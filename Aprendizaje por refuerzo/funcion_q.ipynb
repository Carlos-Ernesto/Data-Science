{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 3.74124712  3.51980981  3.822379    5.49539   ]\n",
      "  [ 4.160187    6.2171      4.57466196  4.28559609]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.1        -0.0199     -0.1        -0.1       ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 5.28424099  7.019       5.20798169  5.06847913]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.019       8.18551063 -0.1145252   0.5529984 ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 5.77668984  5.75330094  5.78617313  7.91      ]\n",
      "  [ 6.70847104  6.21966694  6.70303461  8.9       ]\n",
      "  [ 5.56393417  7.47641467  7.51503791 10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creamos un laberinto representado por una matriz\n",
    "# 0 representa un espacio vacío\n",
    "# 1 representa una pared\n",
    "# 2 representa el estado objetivo\n",
    "# 3 representa el agente\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [3, 0, 1, 0, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 2],\n",
    "    [1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Definimos la función de recompensa\n",
    "def get_reward(state):\n",
    "    if state == 2:  # Objetivo\n",
    "        return 10\n",
    "    elif state == 1:  # Pared\n",
    "        return -1\n",
    "    else:\n",
    "        return -0.1  # Espacio vacío\n",
    "\n",
    "# Función para tomar una acción y obtener el siguiente estado y la recompensa\n",
    "def take_action(state, action):\n",
    "    new_reward = 0  # Inicializamos la recompensa a 0\n",
    "    if action == 0:  # Arriba\n",
    "        next_state = (state[0] - 1, state[1])\n",
    "    elif action == 1:  # Abajo\n",
    "        next_state = (state[0] + 1, state[1])\n",
    "    elif action == 2:  # Izquierda\n",
    "        next_state = (state[0], state[1] - 1)\n",
    "    elif action == 3:  # Derecha\n",
    "        next_state = (state[0], state[1] + 1)\n",
    "\n",
    "    # Comprobar si el siguiente estado está dentro del laberinto\n",
    "    if next_state[0] < 0 or next_state[0] >= maze.shape[0] or next_state[1] < 0 or next_state[1] >= maze.shape[1] or maze[next_state] == 1:\n",
    "        next_state = state  # Mantener el estado actual\n",
    "        new_reward = -1  # Penalizar por chocar con una pared\n",
    "    else:\n",
    "        new_reward = get_reward(maze[next_state])  # Obtener la recompensa del nuevo estado\n",
    "\n",
    "    return next_state, new_reward\n",
    "\n",
    "# Inicializamos la función Q como una matriz de ceros\n",
    "Q = np.zeros((maze.shape[0], maze.shape[1], 4))  # 4 acciones posibles (arriba, abajo, izquierda, derecha)\n",
    "\n",
    "# Definimos parámetros del algoritmo\n",
    "gamma = 0.9  # Factor de descuento\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "\n",
    "# Algoritmo de aprendizaje por refuerzo (Q-Learning)\n",
    "num_episodes = 1000\n",
    "for _ in range(num_episodes):\n",
    "    state = (1, 0)  # Posición inicial del agente\n",
    "    while True:\n",
    "        # Elegir una acción epsilon-greedy\n",
    "        if np.random.rand() < 0.1:\n",
    "            action = np.random.randint(4)  # Exploración aleatoria\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        # Tomar la acción y obtener el siguiente estado y la recompensa\n",
    "        next_state, reward = take_action(state, action)\n",
    "\n",
    "        # Calcular el valor Q según la ecuación de Bellman\n",
    "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "\n",
    "        # Actualizar el estado actual\n",
    "        state = next_state\n",
    "\n",
    "        # Si llegamos al estado objetivo, terminar el episodio\n",
    "        if maze[state] == 2:\n",
    "            break\n",
    "\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
